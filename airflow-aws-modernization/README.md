# ğŸ› ï¸ Legacy Job Orchestration Modernization with Apache Airflow and AWS

This project demonstrates how to modernize legacy Windows-scheduled ETL scripts into robust, scalable Apache Airflow DAGs â€” deployed using Docker and integrated with AWS services like S3 and Redshift.

---
## ğŸ“Š Architecture Overview

![Architecture Diagram](https://github.com/bashoori/repo/blob/master/airflow-aws-modernization/airflow-WS-S3-Redshift.png)

---

## ğŸš€ Features
- Migrate `.bat`/`.py` scripts to Airflow DAGs
- ETL pipeline with extract-transform-load steps
- Data ingestion from public API
- AWS S3 integration
- Dockerized Airflow environment (webserver + scheduler)
- Retry logic, logging, and alerting scaffolding
- Fully documented for learning and reusability

---

## ğŸ“ Folder Structure
```
airflow-aws-modernization/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ legacy_to_airflow_dag.py         # Main Airflow DAG file
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml               # Airflow Docker deployment
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ extract_transform_load.py        # Optional reusable ETL script
â”œâ”€â”€ redshift/
â”‚   â””â”€â”€ create_tables.sql                # Redshift schema setup
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ airflow_aws_architecture.png     # Architecture diagram
â”œâ”€â”€ LICENSE                              # MIT license
â”œâ”€â”€ README.md                            # Project documentation
â”œâ”€â”€ .gitignore                           # Ignore rules for Git
â”œâ”€â”€ .env.example                         # Sample AWS credentials (not committed)
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ airflow-lint.yml             # GitHub Actions CI/CD linting setup
        
```

## ğŸ§± Tech Stack
- Apache Airflow 2.7.1 (Dockerized)
- Python 3.9
- PostgreSQL (Airflow backend)
- AWS S3 (data storage)
- Pandas + Requests + Boto3

---

## ğŸ”§ How to Run

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/airflow-aws-modernization.git
cd airflow-aws-modernization
```

### 2. Set up environment variables
Copy the sample environment file:
```bash
cp .env.example .env
```
Add your AWS credentials to `.env`.

### 3. Start Airflow with Docker Compose
```bash
cd docker
docker-compose up -d
```

### 4. Access the Airflow UI
Go to [http://localhost:8080](http://localhost:8080) and enable the `legacy_to_airflow_dag`.

---

## ğŸ—‚ï¸ DAG Tasks Overview
- **Extract**: Pull product data from a public API
- **Transform**: Add derived fields (e.g., tax-calculated price)
- **Load**: Upload CSV to AWS S3 bucket

---

## ğŸ“Š Architecture
> Diagram available at: `docs/airflow_aws_architecture.png` (to be added)

---

## ğŸ§  About the Author
Built with â¤ï¸ by **Bita Ashoori** â€” Data Engineer & Automation Enthusiast

---

## ğŸ“„ License
Licensed under the [MIT License](LICENSE).

