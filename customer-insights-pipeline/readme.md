# ğŸ“Š Customer Purchase Insights Pipeline

This project demonstrates an end-to-end ETL pipeline that integrates customer transactions from multiple sources: JSON-based online orders, CSV-based in-store transactions, and customer profile data. The pipeline supports both local and S3-based input and outputs a unified FactOrders table ready for analytics.

---

## ğŸš€ Objective

Retail companies often struggle to unify online and in-store sales data alongside CRM records. This project solves that by creating a repeatable, modular pipeline using Python and Pandas that:

- Ingests structured (CSV) and semi-structured (JSON) data
- Cleans and transforms it
- Merges it into a clean star-schema-style model
- Outputs a fact table ready for BI tools

---

## ğŸ—‚ Project Structure

```
customer-insights-pipeline/
â”œâ”€â”€ data/                          # Optional for local testing
â”‚   â”œâ”€â”€ mock_customers.csv
â”‚   â”œâ”€â”€ mock_store_sales.csv
â”‚   â””â”€â”€ mock_online_orders.json
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ fact_orders.csv
    â””â”€â”€ fact_orders_from_s3.csv
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ etl_pipeline.py           # Local file-based ETL
â”‚   â””â”€â”€ etl_pipeline_s3.py        # S3-based ETL with boto3
â”œâ”€â”€ requirements.txt
â””â”€â”€ .devcontainer/
    â””â”€â”€ devcontainer.json
```

---

## ğŸ”§ Tech Stack

- Python 3.10
- Pandas
- Boto3 (for S3 integration)
- JSON & CSV handling
- GitHub Codespaces (DevContainer)

---

## âš™ï¸ ETL Flow

### Local Version (`etl_pipeline.py`)
1. Load files from `/data`
2. Flatten JSON and clean columns
3. Join with customer master
4. Output to `/outputs/fact_orders.csv`

### S3 Version (`etl_pipeline_s3.py`)
1. Load files from:
   ```
   s3://bashoori-s3/customer-purchase-data/
   ```
2. Transform and join records
3. Save unified dataset to `outputs/fact_orders.csv`

---

## ğŸ“„ Output Sample: `fact_orders.csv`

| order_id | customer_id | product_id | quantity | price | timestamp | total_value | region     | customer_segment |
|----------|-------------|------------|----------|--------|-----------|-------------|------------|------------------|
| O2000    | C1001       | P101       | 2        | 59.99  | ...       | 119.98      | East Coast | Silver           |

---

## ğŸ§ª How to Run (S3 Version)

1. Ensure AWS CLI is installed:
   ```bash
   sudo apt install awscli
   ```

2. Configure your credentials:
   ```bash
   aws configure
   ```

3. Run:
   ```bash
   cd scripts
   python etl_pipeline_s3.py
   ```

---

## ğŸ“¦ Requirements

Install dependencies:
```bash
pip install -r requirements.txt
```

---

## ğŸ’» Dev Environment

Supports GitHub Codespaces with pre-configured Python environment:
```
.devcontainer/devcontainer.json
```

---

## âœï¸ Author

**Bita Ashoori**  
_Data Engineer & Data Automation Specialist_  
ğŸ“ [GitHub](https://github.com/bashoori) | [LinkedIn](https://www.linkedin.com/in/bashoori)

---

## â­ï¸ Show Your Support

If you found this useful, feel free to star the repo or share feedback!


---

## ğŸ“¥ Data Input Options

This pipeline supports **two modes of input**, allowing it to run both in development (GitHub Codespaces) and production-like (cloud) environments.

### 1. ğŸŸ¢ S3-Based Input (Cloud Mode)
The script `etl_pipeline_s3.py` reads input files from your AWS S3 bucket:
```
s3://bashoori-s3/customer-purchase-data/
```

Files expected in the bucket:
- `mock_customers.csv`
- `mock_store_sales.csv`
- `mock_online_orders.json`

Run using:
```bash
cd scripts
python etl_pipeline_s3.py
```

### 2. ğŸ§ª Local Input (Development Mode)
The script `etl_pipeline.py` reads files from the local `data/` folder. This is great for GitHub Codespaces or local testing.

Files should be located at:
```
data/
â”œâ”€â”€ mock_customers.csv
â”œâ”€â”€ mock_store_sales.csv
â””â”€â”€ mock_online_orders.json
```

Run using:
```bash
cd scripts
python etl_pipeline.py
```

Both scripts output the same result:  
ğŸ“„ `outputs/fact_orders.csv`

