# ğŸŒ©ï¸ Cloud ETL Modernization with Apache Airflow & AWS

An end-to-end **ETL pipeline** project using **Apache Airflow** for workflow orchestration and **AWS (Redshift, S3)** for scalable cloud storage and data loading. This project simulates a modern, production-ready data engineering environment that ingests mock API data, transforms it, and loads it into a cloud data warehouse.

---

## ğŸ“Œ Purpose

This project showcases:

- How to orchestrate real-world ETL workflows using Airflow
- Modular task design with Python operators
- Cloud integration with AWS S3 and Redshift
- Local development inside GitHub Codespaces using Docker and DevContainers

---

## âš™ï¸ Architecture Overview
```
     +------------+        +------------------+       +------------------+
     | Mock API   | -----> | Airflow DAG:     | ----> | transform_ads.py |
     | (JSON Ads) |        | fetch_ads_data   |       +------------------+
     +------------+                                     |
                                                         v
                                                  +------------------+
                                                  | load_to_redshift |
                                                  +------------------+
                                                         |
                                                         v
                                                +--------------------+
                                                | AWS Redshift Table |
                                                +--------------------+
```
 ---

## ğŸ§° Tech Stack

| Category                | Tools & Technologies                                                  |
|-------------------------|-----------------------------------------------------------------------|
| **Language**            | Python 3.9                                                            |
| **Orchestration**       | Apache Airflow (v2.7.3)                                               |
| **Environment**         | Docker + DevContainer + GitHub Codespaces                            |
| **ETL Scripting**       | Custom Python scripts (`/scripts`) for ingestion, transformation     |
| **Storage (Mocked)**    | AWS S3 (emulated via local directory), AWS Redshift (via placeholder) |
| **Data Format**         | JSON (mock API), CSV (transformed), Pandas DataFrames                |
| **Scheduler**           | Airflow's built-in scheduler & CLI-triggered DAGs                    |
| **Development**         | VS Code + Remote Containers + GitHub                                 |

---

## ğŸ“‚ Project Structure
```
cloud-etl-modernization-airflow-aws/
â”‚
â”œâ”€â”€ .devcontainer/                # DevContainer setup for Codespaces
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ devcontainer.json
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ dags/                         # Airflow DAGs
â”‚   â””â”€â”€ api_ingestion_dag.py
â”‚
â”œâ”€â”€ mock_data/                    # Simulated API data
â”‚   â””â”€â”€ ads_data.json
â”‚
â”œâ”€â”€ scripts/                      # ETL scripts
â”‚   â”œâ”€â”€ transform_ads_data.py
â”‚   â””â”€â”€ load_to_redshift.py
â”‚
â”œâ”€â”€ docker-compose.yml           # Airflow service definitions
â””â”€â”€ README.md
```
---

## ğŸ“Œ Notable Airflow DAGs

This project includes modular Airflow DAGs designed for modern ETL workflows. Each DAG is focused on a specific stage of the data pipeline, from data ingestion to transformation and loading.

### DAG: `api_ingestion_dag`

An end-to-end DAG that orchestrates the entire pipeline:

1. **Ingests** mock advertisement data (JSON) from a local mock source
2. **Transforms** raw JSON into structured tabular format
3. **Loads** cleaned data into AWS Redshift (or mock DB in development)

#### Tasks Breakdown

| Task ID            | Description                                | Operator       |
|--------------------|--------------------------------------------|----------------|
| `fetch_ads_data`   | Reads and parses JSON ad data              | PythonOperator |
| `transform_ads`    | Cleans and structures the ads data         | PythonOperator |
| `load_to_redshift` | Writes the transformed data to Redshift    | PythonOperator |

#### DAG Config

- **Schedule**: `@daily`
- **Retries**: `1` (with delay)
- **Owner**: `bita`
- **Dependencies**: `transform_ads` depends on `fetch_ads_data`; `load_to_redshift` runs last.

---

## ğŸš€ Features

- ğŸŒ€ Modular DAGs: `fetch_ads_data`, `transform_ads`, `load_to_redshift`
- ğŸ³ Dockerized with Airflow 2.7 for easy reproducibility
- ğŸ§ª Devcontainer-ready for GitHub Codespaces
- ğŸ“¥ Mock API input via local files (mock_data)
- ğŸ§¹ Transformation logic via Python scripts
- ğŸ”º Cloud Output to AWS Redshift (simulated)

---

## â–¶ï¸ How to Run

### Local Setup

```bash
git clone https://github.com/bashoori/cloud-etl-modernization-airflow-aws
cd cloud-etl-modernization-airflow-aws

# Launch Airflow using Docker Compose
docker-compose up --build

Then visit: http://localhost:8080
	â€¢	Default credentials: admin / admin (or use CLI to create)

GitHub Codespaces (Recommended)

Just open the Codespace and it will:
	â€¢	Start the container
	â€¢	Run Airflow migrations
	â€¢	Create admin user
	â€¢	Launch scheduler & webserver

No setup needed!

â¸»

ğŸ“ˆ Future Enhancements
	â€¢	Connect to real APIs (e.g. Facebook Ads, Google Analytics)
	â€¢	Replace SQLite with PostgreSQL for dev
	â€¢	Add dbt for transformations
	â€¢	Add data quality tests and email alerts

â¸»

ğŸ™‹â€â™€ï¸ Author

Bita Ashoori
GitHub: bashoori
LinkedIn: linkedin.com/in/bashoori

ğŸ’¡ This project is part of my Data Engineering Portfolio
